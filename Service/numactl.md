# numactl

## 简介
NUMA（Non-Uniform Memory Access，非一致性内存访问）是一种计算机架构设计，用于解决多处理器系统中的内存访问延迟和带宽问题。

在传统的对称多处理器（SMP）系统中，所有处理器共享同一个内存空间，每个处理器可以直接访问任何内存位置。这种设计适用于处理器数量较少的系统，但在大规模多处理器系统中，由于内存控制器和处理器之间的物理距离和带宽限制，访问远程内存的延迟会显著增加，导致性能下降。

NUMA 架构通过将内存分成多个区域（或节点），每个节点与特定的处理器组件相关联。每个处理器只能直接访问本地节点的内存，而对于远程节点的内存访问则需要通过互连网络进行。这种架构使得内存访问更接近于本地访问，减少了访问延迟，提高了系统的性能和可扩展性。

NUMA 架构还提供了一些优化策略，如数据局部性、任务和数据的分布、缓存一致性等，以进一步提高系统性能。这些优化策略需要在操作系统和应用程序层面进行支持和配置。

总而言之，NUMA 架构通过将内存划分为多个节点，并将每个节点与特定的处理器关联，以解决多处理器系统中的内存访问延迟和带宽问题，提高系统性能和可扩展性。

## 例子
1. 绑定进程到特定的节点：
   ```
   numactl --cpunodebind=1 --membind=1 <command>
   ```
   这个命令将 `<command>` 绑定到节点 1，即指定进程只能在节点 1 上运行，并且只能访问节点 1 上的内存。

2. 显示系统中的节点信息：
   ```
   numactl --hardware
   ```
   这个命令将显示系统中的节点拓扑结构、节点的物理 ID、处理器和内存的分布情况等信息。

3. 查看进程的 NUMA 亲和性：
   ```
   numactl --show
   ```
   这个命令将显示当前进程的 NUMA 亲和性，即进程所绑定的节点和内存。

4. 在特定节点上运行命令：
   ```
   numactl --cpunodebind=2 --membind=2 <command>
   ```
   这个命令将 `<command>` 在节点 2 上运行，并且只能访问节点 2 上的内存。

5. 绑定进程到特定的 CPU：
   ```
   numactl --physcpubind=0-3 <command>
   ```
   这个命令将 `<command>` 绑定到 CPU 0 到 3 上运行。


## 查看节点
使用`numactl --hardware`命令来查看系统中的节点信息，包括 CPU 节点和内存节点的数量。

运行以下命令来显示系统中的节点信息：

```bash
numactl --hardware
```

该命令将显示节点的物理 ID、节点的大小、节点上的处理器和内存信息。在输出中，您可以查找类似于 `node <node_id> size: <size> MB` 的行，其中 `<node_id>` 就是节点的 ID，`<size>` 是节点的大小（以 MB 为单位）。通过统计节点的数量，您可以得知系统中的 CPU 节点和内存节点的数量。

请注意，节点的数量可能因不同的系统而有所不同。因此，具体的输出可能会根据您的系统而有所变化。


```bash
$ numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
node 0 size: 64075 MB
node 0 free: 4306 MB
node 1 cpus: 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111
node 1 size: 64457 MB
node 1 free: 2738 MB
node distances:
node   0   1
  0:  10  21
  1:  21  10
```

## "node distances" 表示节点之间的距离。
它显示了不同节点之间的相对距离，用于表示节点之间的访问延迟。

节点之间的距离信息通常以一个矩阵的形式呈现，其中每个元素表示节点之间的相对距离。这些距离值可以是离散的，也可以是连续的，具体取决于硬件架构和节点之间的连接方式。

节点距离的值越小，表示节点之间的距离越近，访问延迟也越低。相反，节点距离的值越大，表示节点之间的距离越远，访问延迟也越高。

节点距离信息对于 NUMA 架构非常重要，因为它可以帮助操作系统和应用程序决定在哪个节点上分配任务和数据，以最小化远程访问和延迟。通常情况下，节点距离信息是由硬件提供，并在操作系统中进行配置和管理。

请注意，节点距离信息的具体解释和使用可能因不同的硬件和操作系统而有所不同。用户在使用节点距离信息时，应该了解其具体含义，并根据实际情况进行优化和配置。


## lspci 里面的 NUMA 节点
当使用 `lspci` 命令查看系统中的 PCI 设备信息时，你可能会看到每个设备的 "NUMA node" 字段。这个字段表示与每个 PCI 设备关联的 NUMA 节点。
```bash
$ sudo lspci -vv -s "5f:00.0" | grep -i numa
        NUMA node: 0
```

在 NUMA 架构中，系统的内存和处理器被组织成多个节点，每个节点具有自己的本地内存和处理器。PCI 设备通常与特定的 NUMA 节点关联，这意味着这些设备更接近于某个节点，而不是整个系统。

使用 `lspci` 命令可以查看每个 PCI 设备的详细信息，其中包括设备的 NUMA 节点。NUMA 节点的编号与系统中的节点编号相对应，可以帮助你了解每个设备所关联的节点，以及设备与内存和处理器之间的距离。

对于 NUMA 架构的系统，了解设备与 NUMA 节点之间的关系可以帮助你优化设备的性能。例如，你可以将与特定设备关联的任务和数据分配到该设备所在的 NUMA 节点上，以减少远程访问和延迟。

请注意，NUMA 节点的编号可能因不同的系统而有所不同。因此，在使用 `lspci` 命令查看设备的 NUMA 节点时，要根据系统的实际情况进行解释和使用。



